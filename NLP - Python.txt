###########################
# Python Cheat Sheet for NLP
###########################

### 1. Text Preprocessing
- Tokenization:
  from nltk.tokenize import word_tokenize
  tokens = word_tokenize(text)
  
- Stopword Removal:
  from nltk.corpus import stopwords
  stopwords = set(stopwords.words('english'))
  filtered_tokens = [word for word in tokens if word.lower() not in stopwords]
  
- Stemming (Porter Stemmer):
  from nltk.stem import PorterStemmer
  stemmer = PorterStemmer()
  stemmed_tokens = [stemmer.stem(word) for word in filtered_tokens]

- Lemmatization:
  from nltk.stem import WordNetLemmatizer
  lemmatizer = WordNetLemmatizer()
  lemmatized_tokens = [lemmatizer.lemmatize(word) for word in filtered_tokens]

### 2. Feature Extraction
- TF-IDF Vectorization:
  from sklearn.feature_extraction.text import TfidfVectorizer
  vectorizer = TfidfVectorizer(max_features=1000)
  X = vectorizer.fit_transform(corpus)

- Word Embeddings (Word2Vec):
  from gensim.models import Word2Vec
  model = Word2Vec(sentences, vector_size=100, window=5, min_count=1, workers=4)

- Contextual Embeddings (BERT):
  from transformers import BertTokenizer, BertModel
  tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')
  model = BertModel.from_pretrained('bert-base-uncased')

### 3. Text Classification
- Training a Classifier (SVM):
  from sklearn.svm import SVC
  classifier = SVC(kernel='linear')
  classifier.fit(X_train, y_train)

- Evaluation Metrics (Accuracy):
  from sklearn.metrics import accuracy_score
  y_pred = classifier.predict(X_test)
  accuracy = accuracy_score(y_test, y_pred)

### 4. Named Entity Recognition (NER)
- Using SpaCy:
  import spacy
  nlp = spacy.load('en_core_web_sm')
  doc = nlp(text)
  for ent in doc.ents:
      print(ent.text, ent.label_)

### 5. Sentiment Analysis
- VADER Sentiment Analysis:
  from nltk.sentiment.vader import SentimentIntensityAnalyzer
  analyzer = SentimentIntensityAnalyzer()
  sentiment_scores = analyzer.polarity_scores(text)

### 6. Topic Modeling
- Latent Dirichlet Allocation (LDA):
  from sklearn.decomposition import LatentDirichletAllocation
  lda = LatentDirichletAllocation(n_components=5, random_state=42)
  lda.fit(X)

### 7. Sequence-to-Sequence Tasks
- Machine Translation (Seq2Seq):
  from transformers import MarianMTModel, MarianTokenizer
  model_name = 'Helsinki-NLP/opus-mt-en-ro'
  tokenizer = MarianTokenizer.from_pretrained(model_name)
  model = MarianMTModel.from_pretrained(model_name)

### 8. Model Deployment
- Flask API Deployment:
  from flask import Flask, request, jsonify
  app = Flask(__name__)

  @app.route('/predict', methods=['POST'])
  def predict():
      text = request.json['text']
      # Perform NLP task
      return jsonify({'result': result})

  if __name__ == '__main__':
      app.run(debug=True)

### Additional Resources
- NLTK Documentation: https://www.nltk.org/
- SpaCy Documentation: https://spacy.io/
- Transformers Documentation: https://huggingface.co/transformers/

### Tips and Best Practices
- Use GPU for training large models (CUDA support).
- Experiment with different hyperparameters and models.
- Consider model interpretability and bias mitigation.

